{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least-Squares Fitting\n",
    "Here is an introduction to fitting a function $f(x)$ to data $y$ using what is called least-squares minimization. The idea is to evaluate the distance between function values and data values and minimize this distance. In the image below, two different distances are shown. In this example, we will calculate the vertical distance.\n",
    "\n",
    "![Vertical and perpendicular distances between function and data https://mathworld.wolfram.com/LeastSquaresFitting.html](https://github.com/troymessina/ModernPhysics/blob/master/Learn%20Jupyter/least_squares_offsets.gif?raw=true)\n",
    "Image source: https://mathworld.wolfram.com/LeastSquaresFitting.html\n",
    "\n",
    "The vertical distance is simply the difference between the function value calculated at the particular $x$ the data value at the particular $x$.\n",
    "\n",
    "$$D_i = f(x_i) - y_i$$\n",
    "\n",
    "where $i$ denotes the particular $x$ chosen and $y$ is the data value. One issue that arises is that some values of $D$ are positive, and some are negative. If we try to total the difference between the function and the data points by adding them all together, we might get a positive, negative, or zero value as a result. A perfect match between the function and the data is when the total difference between the function and data is zero. Therefore, we would like the total difference to *minimize to zero*. This means negative totals will cause problems. The way we get around this is to square the differences and minimize the sum of those squares. Hence the name \"Least-Squares\". So, we define the distance as\n",
    "\n",
    "$$D_i = \\left(f(x_i) - y_i\\right)^2$$\n",
    "\n",
    "We will have an array of $D$ values. There will be a $D_i$ for each data point $y_i$. In order to minimize the total distance, we need to sum all of the $D_i$ values and minimize the sum by adjusting our guesses at constants in the function. These constants are often called \"fit parameters\". For example, the function of a line has fit parameters of slope and intercept that we can adjust to create the best fit line to our data. However, we do one more step so that the quantity we minimize has the same units as the data, and we divide the sum of $D_i$ by the function value $f(x_i)$, this quantity has the name chi-squared ($\\chi^2$).\n",
    "\n",
    "$$\\chi^2 = \\sum_i \\frac{\\left(f(x_i)-y_i\\right)^2}{f(x_i)}$$\n",
    "\n",
    "This is the number we want to minimize. Let's take a look by doing an example. First, import some libraries for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #for math\n",
    "import matplotlib.pyplot as plt #for graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make some data\n",
    "Next, we will create a set of linear data. We will create data that we know the correct slope and intercept so that we can easily implement the process. Your job is to\n",
    "\n",
    "1. create a known slope and intercept between 1 and 3.\n",
    "2. insert values to make the `x` values as stated in the comment.\n",
    "3. insert the equation of a line for the data `y`.\n",
    "\n",
    "The lines of code after the equation of the line will make your $y$ data noisy so that it is like it came from an experiment. Because this is a random process, you do <strong>NOT</strong> want to run this section more than once before completing the activity because it will change the data each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a slope and intercept\n",
    "m = \n",
    "b = \n",
    "\n",
    "#make x-values from 0 to 5 in steps of 1\n",
    "x = np.linspace(start, stop, numpnts)\n",
    "\n",
    "#insert your linear function\n",
    "y = \n",
    "\n",
    "#add some noise to the data\n",
    "noise = np.random.rand(np.size(x))-0.5 #make noise array with values between -0.5 and 0.5\n",
    "y+= y*noise/5 #add noise to the data that is +/- 10% of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Check\n",
    "We can graph the data to make sure it looks like we think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'bo', label='Data')\n",
    "plt.grid(True)\n",
    "plt.legend(loc=0)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing the Least-Squares\n",
    "Looks good. Now, we will calculate the distances between this $y$ data and the function\n",
    "\n",
    "$$f(x) = mx+b$$\n",
    "\n",
    "Here is what you should do in the following code cell.\n",
    "\n",
    "1. Make variables for guessing the slope and intercept.\n",
    "    - call these `m_guess` and `b_guess`.\n",
    "2. Make an array that has $f(x)$ called `f_of_x` using `x` and the slope and intercept guesses.\n",
    "    - Remember that math on arrays returns arrays of the same size.\n",
    "3. Create a variable `chi_square_sum` that is equal to zero.\n",
    "    - We are going to add each distance and want to make sure this variable is zero to begin.\n",
    "4. Create the array named `chi_square` that is equal to the $\\chi^2$ function above.\n",
    "\n",
    "The next parts of the code below are a loop that goes through the `chi_square` distance array, adds each value to `chi_square_sum`. Finally, we print the value of $\\chi^2$.\n",
    "\n",
    "Your task is to find the best-fit values of slope and intercept to two decimal places. You will do this by adjusting your guesses and repeatedly running the cell below until you minimize $\\chi^2$. Since we added noise to the data, the best fit values will not be the ones you used to create the data, but those values you used are a starting guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guesses for the fit parameters.\n",
    "\n",
    "\n",
    "#define f(x) using the guesses and x array\n",
    "\n",
    "\n",
    "#initialize chi_sq_sum to zero\n",
    "\n",
    "\n",
    "#chi squared\n",
    "\n",
    "\n",
    "for i in range(0, np.size(chi_square)):\n",
    "    \n",
    "    \n",
    "print(chi_sq_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Final Plot of Everything\n",
    "Once we have minimized $\\chi^2$, it is time to plot our data with the best fit. In the cell below,\n",
    "\n",
    "1. Plot the data (`y` vs. `x`) as markers and color of your choice.\n",
    "2. Plot the best fit line `f_of_x` as a solid line.\n",
    "3. Add a legend with `Data` and `Fit` as labels.\n",
    "4. Add axis labels.\n",
    "\n",
    "See above if you have forgotten the plotting commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
